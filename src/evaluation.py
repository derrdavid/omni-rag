from openai import OpenAI
from dotenv import load_dotenv
from pydantic import BaseModel
from main import omni_rag
import psycopg2
import os
import json

class EvaluationOutput(BaseModel):
    evaluation_notes: str
    evaluation_score: int

load_dotenv()
client = OpenAI()

def generate_pseudo(query, docs):
    context = "\n".join(d.contents for d in docs)
    result = client.responses.create(
        model="gpt-4o",
        input=[{"role": "developer", "content": f"Context: {context}"}, {"role": "user", "content": query}]
    )
    return result.output_text

def generate_relevance_evaluation(question, answer, pseudo_answer, docs):
    system_prompt = f"""
        You are an expert evaluator for RetrievalAugmented Generation (RAG) systems.
        Your task is to assess the quality of responses
        generated by a RAG system based on the relevance
        (correctness) criteria:
        Relevance - Measures the correctness and relevance
        of the answer to the question on a four-point
        scale:
        2: The response correctly answers the user
        question and contains no irrelevant content
        1: The response provides a useful answer to
        the user question, but may contain irrelevant
        content that do not harm the usefulness of
        the answer
        0: No answer is provided in the response (e.g.,
        "I don't know")
        -1: The response does not answer the question
        whatsoever
        You will be provided with:
        - A question
        - The response generated by the RAG system
        - The retrieved documents used as context
        - A gold reference answer (if available)
        When a gold reference answer is provided, use
        it as an additional reference point for evaluating
        the correctness and completeness of the RAG
        system's response. The gold reference represents
        an ideal answer to the question.
        Provide your evaluation in a structured JSON
        format with the following fields:
        - evaluation_notes: Brief explanation of your
        reasoning for each score
        - relevance_score: The relevance score (-1,
        0, 1, or 2)
        Be objective and thorough in your assessment.
        Focus on whether the response correctly answers
        the question.
    """
    user_prompt = f"""
        Please evaluate the following RAG system response:
        QUESTION:
        {question}
        RESPONSE:
        {answer}
        GOLD REFERENCE ANSWER:
        {pseudo_answer}
        RETRIEVED DOCUMENTS:
        {docs}
        Based on the above, please evaluate the response
        on relevance (2, 1, 0, or -1).
        Provide your evaluation in the following JSON
        format:
        ```json
        {{
            "evaluation_notes": "[your reasoning in a
        single paragraph]",
        "evaluation_score": [score]
        }}
        ```
    """
    result = client.responses.parse(
        model="gpt-4o",
        input=[{"role": "developer", "content": system_prompt}, {"role": "user", "content": user_prompt}],
        text_format=EvaluationOutput
    )
    return result.output_parsed

def generate_faithfulness_evaluation(question, answer, pseudo_answer, docs):
    system_prompt = f"""
        You are an expert evaluator for Retrieval-
        Augmented Generation (RAG) systems.
        Your task is to assess the quality of responses
        generated by a RAG system based on the faithfulness
        (support) criteria:
        Assess whether the response is grounded in
        the retrieved passages on a three-point scale:
        1: Full support, all answer parts are grounded
        0: Partial support, not all answer parts are
        grounded
        -1: No support, all answer parts are not grounded
        You will be provided with:
        - A question
        - The response generated by the RAG system
        - The retrieved documents used as context
        Provide your evaluation in a structured JSON
        format with the following fields:
        - evaluation_notes: Brief explanation of your
        reasoning for each score
        - faithfulness_score: The faithfulness score
        (-1, 0, or 1)
        Be objective and thorough in your assessment.
        Focus on whether the response correctly answers
        the question and is supported by the retrieved
        documents.
    """
    user_prompt = f"""
        Please evaluate the following RAG system response:
        QUESTION:
        {question}
        RESPONSE:
        {answer}
        GOLD REFERENCE ANSWER:
        {pseudo_answer}
        RETRIEVED DOCUMENTS:
        {docs}
        Based on the above, please evaluate the response
        on faithfulness (1, 0, or -1).
        Provide your evaluation in the following JSON
        format:
        ```json
        {{
        "evaluation_notes": "[your reasoning in a
        single paragraph]",
        "evaluation_score": [score]
        }}
```
    """
    result = client.responses.parse(
        model="gpt-4o",
        input=[{"role": "developer", "content": system_prompt}, {"role": "user", "content": user_prompt}],
        text_format=EvaluationOutput
    )
    return result.output_parsed

"""
# Synthethic Question Generation
"""
def get_all_contents():
    conn = psycopg2.connect(os.getenv("TIMESCALE_SERVICE_URL"))
    cur = conn.cursor()
    cur.execute("SELECT contents FROM vectors")
    results = [row[0] for row in cur.fetchall()]
    cur.close()
    conn.close()
    return results

def generate_question_from_row():
    contents = get_all_contents()
    questions_file_path = os.path.join(os.path.dirname(__file__), "eval", "eval_questions.json")
    system_prompt = """
        Generate a single-line, noisy user query with multiple intents, including spelling errors and ambiguity, suitable for testing a retrieval-augmented generation system in an open-domain setting.
    """
    with open(questions_file_path, "w", encoding="utf-8") as json_file:
        json_file.write("[\n")
        for idx, content in enumerate(contents):
            result = client.responses.create(
                model="gpt-4o",
                input=[{"role": "developer", "content": system_prompt}, {"role": "user", "content": content}],
            )
            question_obj = {"index": idx, "question": result.output_text.strip()}
            json.dump(question_obj, json_file, ensure_ascii=False)
            if idx < len(contents) - 1:
                json_file.write(",\n")
            else:
                json_file.write("\n")
            print(f"Processed {idx + 1} of {len(contents)}")
        json_file.write("]\n")


def start_evaluation():
    questions_file_path = os.path.join(os.path.dirname(__file__), "eval", "eval_questions.json")
    eval_output_path = os.path.join(os.path.dirname(__file__), "eval", "eval_results.json")
    with open(questions_file_path, "r", encoding="utf-8") as f:
        questions = json.load(f)

    # Erstelle die JSON-Datei und schreibe den Anfang des Arrays
    with open(eval_output_path, "w", encoding="utf-8") as f:
        f.write("[\n")

    # Sammle die Scores für die Zusammenfassung
    total_relevance_score = 0
    total_faithfulness_score = 0

    for idx, q in enumerate(questions):
        question = q["question"]

        gen_output = omni_rag(question)
        answer = gen_output.generation_output
        docs = gen_output.reranked_docs

        pseudo_answer = generate_pseudo(question, docs)

        relevance_eval = generate_relevance_evaluation(
            question, answer, pseudo_answer, [d.contents for d in docs]
        )
        faithfulness_eval = generate_faithfulness_evaluation(
            question, answer, pseudo_answer, [d.contents for d in docs]
        )

        # Summiere die Scores
        total_relevance_score += relevance_eval.evaluation_score
        total_faithfulness_score += faithfulness_eval.evaluation_score

        result = {
            "index": q["index"],
            "question": question,
            "answer": answer,
            "pseudo_answer": pseudo_answer,
            "relevance_evaluation": relevance_eval.model_dump(),
            "faithfulness_evaluation": faithfulness_eval.model_dump()
        }

        print(f"Schreibe Ergebnis {idx + 1}/{len(questions)} in Datei...")
        
        # Schreibe das Ergebnis sofort in die Datei
        with open(eval_output_path, "a", encoding="utf-8") as f:
            # Konvertiere zu JSON mit korrekter Einrückung
            json_str = json.dumps(result, ensure_ascii=False, indent=2)
            # Füge Einrückung für das gesamte Objekt hinzu (2 Leerzeichen)
            indented_json = '\n'.join('  ' + line for line in json_str.split('\n'))
            f.write(indented_json)
            f.write(",\n")  # Komma nach jedem Ergebnis

    # Erstelle das Summary-Objekt
    summary = {
        "summary": {
            "total_questions": len(questions),
            "total_relevance_score": total_relevance_score,
            "total_faithfulness_score": total_faithfulness_score,
            "average_relevance_score": round(total_relevance_score / len(questions), 2) if len(questions) > 0 else 0,
            "average_faithfulness_score": round(total_faithfulness_score / len(questions), 2) if len(questions) > 0 else 0
        }
    }

    print("Schreibe Summary...")
    
    # Schreibe das Summary-Objekt
    with open(eval_output_path, "a", encoding="utf-8") as f:
        json_str = json.dumps(summary, ensure_ascii=False, indent=2)
        indented_json = '\n'.join('  ' + line for line in json_str.split('\n'))
        f.write(indented_json)
        f.write("\n")

    # Schließe das JSON-Array
    with open(eval_output_path, "a", encoding="utf-8") as f:
        f.write("]\n")

    print(f"Evaluation abgeschlossen! Ergebnisse in {eval_output_path} gespeichert.")
    print(f"Gesamt Relevance Score: {total_relevance_score}")
    print(f"Gesamt Faithfulness Score: {total_faithfulness_score}")
    print(f"Durchschnitt Relevance: {round(total_relevance_score / len(questions), 2)}")
    print(f"Durchschnitt Faithfulness: {round(total_faithfulness_score / len(questions), 2)}")
      
if __name__ == "__main__":
    start_evaluation()

